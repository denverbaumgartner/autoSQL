{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from replicate import Client as rc\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data import SQLData\n",
    "from predict import SQLPredict\n",
    "from eval import SQLEval\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Now you can get the loaded environment variable using os.environ\n",
    "GITHUB_GIST_TOKEN = os.environ.get(\"GITHUB_GIST_TOKEN\")\n",
    "REPLICATE_API_TOKEN = os.environ.get(\"REPLICATE_API_TOKEN\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "MODEL_VERSION = os.environ.get(\"REPLICATE_LLAMA_13B_BASE\")\n",
    "MODEL_DESINATION = os.environ.get(\"REPLICATE_LLAMA_13B_TUNE\")\n",
    "REPLICATE_LLAMA_13B_TUNED = os.environ.get(\"REPLICATE_LLAMA_13B_TUNED\")\n",
    "\n",
    "dataset_name = 'b-mc2/sql-create-context'\n",
    "train_test_dataset = \"data_llama_13b_1_0_1\"\n",
    "replicate = rc(REPLICATE_API_TOKEN)\n",
    "\n",
    "data_upload = False\n",
    "run_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqp = SQLPredict.from_replicate_model(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    replicate_api_key=REPLICATE_API_TOKEN,\n",
    "    model_name=\"llama_2_13b_sql\",\n",
    "    model_id=REPLICATE_LLAMA_13B_TUNED\n",
    ")\n",
    "sqd = SQLData.from_sql_create_context()\n",
    "sqe = SQLEval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqd.preprocess_data(dataset_name=dataset_name)\n",
    "sqd.filter_data(dataset_name=dataset_name)\n",
    "sqd.train_test_split(dataset_name=dataset_name, new_dataset_name=train_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_upload:\n",
    "    training_resp = sqd.upload_jsonl_gist(\n",
    "        dataset_name=train_test_dataset,\n",
    "        token=GITHUB_GIST_TOKEN,\n",
    "        filename=\"training_data_llama_13b_1_0_1.jsonl\",\n",
    "        description=\"Training data for the second version of the llama_2_13b_sql (1.0.1) model\",\n",
    "    )\n",
    "    training_data_url = training_resp['files']['training_data_llama_13b_1_0_1.jsonl']['raw_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_upload:\n",
    "    testing_resp = sqd.upload_jsonl_gist(\n",
    "        dataset_name=train_test_dataset,\n",
    "        token=GITHUB_GIST_TOKEN,\n",
    "        filename=\"testing_data_llama_13b_1_0_1.jsonl\",\n",
    "        dataset_type=\"test\",\n",
    "        description=\"Testing data for the first version of the llama_2_13b_sql (1.0.1) model\",\n",
    "    )\n",
    "    testing_data_url = testing_resp['files']['testing_data_llama_13b_1_0_1.jsonl']['raw_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    training = replicate.trainings.create(\n",
    "        version= MODEL_VERSION,\n",
    "        input={\n",
    "            \"train_data\": training_data_url,\n",
    "            \"num_train_epochs\": 3,\n",
    "        },\n",
    "        destination=MODEL_DESINATION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing = sqd.data[train_test_dataset]['test'].filter(lambda example: example['query_result'] != '[(0,)]' and example['query_result'] != '[(None,)]')\n",
    "\n",
    "# run in batches of 100 to evaluate time and to avoid timeouts\n",
    "# run in separate cells in case of timeouts\n",
    "rich_testing_subset_0_100 = rich_testing.select(range(0, 100))\n",
    "rich_testing_subset_100_200 = rich_testing.select(range(100, 200))\n",
    "rich_testing_subset_200_300 = rich_testing.select(range(200, 300))\n",
    "\n",
    "rich_testing_subset_0_100 = rich_testing_subset_0_100.map(sqd.format_tuning_data)\n",
    "rich_testing_subset_100_200 = rich_testing_subset_100_200.map(sqd.format_tuning_data)\n",
    "rich_testing_subset_200_300 = rich_testing_subset_200_300.map(sqd.format_tuning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_0_100 = rich_testing_subset_0_100.map(sqp.openai_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_100_200 = rich_testing_subset_100_200.map(sqp.openai_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_200_300 = rich_testing_subset_200_300.map(sqp.openai_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_0_100 = rich_testing_subset_0_100.map(sqp.replicate_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_100_200 = rich_testing_subset_100_200.map(sqp.replicate_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset_200_300 = rich_testing_subset_200_300.map(sqp.replicate_dataset_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset = datasets.concatenate_datasets([rich_testing_subset_0_100, rich_testing_subset_100_200, rich_testing_subset_200_300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_testing_subset.save_to_disk(\"../local_data/rich_testing_subset_llama_13b_1_0_0_inferences_three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "### See the eval.ipynb notebook for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = rich_testing_subset.map(sqe.validate_replicate_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = inference_data.map(sqe.replicate_response_parser)\n",
    "inference_data = inference_data.map(sqe.validate_openai_query)\n",
    "inference_data = inference_data.map(sqe.validate_replicate_query)\n",
    "inference_data = inference_data.map(sqe.inference_result_check)\n",
    "\n",
    "openai_valid_queries = inference_data.filter(lambda x: x[\"openai_valid\"] == True)\n",
    "replicate_valid_queries = inference_data.filter(lambda x: x[\"replicate_valid\"] == True)\n",
    "\n",
    "openai_valid_results = inference_data.filter(lambda x: x[\"openai_correct\"] == True)\n",
    "replicate_valid_results = inference_data.filter(lambda x: x[\"replicate_correct\"] == True)\n",
    "\n",
    "print(\"OpenAI valid queries: {}\".format(openai_valid_queries.num_rows))\n",
    "print(\"Replicate valid queries: {}\".format(replicate_valid_queries.num_rows))\n",
    "\n",
    "print(\"OpenAI valid results: {}\".format(openai_valid_results.num_rows))\n",
    "print(\"Replicate valid results: {}\".format(replicate_valid_results.num_rows))\n",
    "\n",
    "print(\"\\nAs a percentage of total queries: \\n\")\n",
    "print(\"OpenAI valid queries: {:.2f}%\".format(100 * openai_valid_queries.num_rows / inference_data.num_rows))\n",
    "print(\"Replicate valid queries: {:.2f}%\".format(100 * replicate_valid_queries.num_rows / inference_data.num_rows))\n",
    "\n",
    "print(\"OpenAI valid results: {:.2f}%\".format(100 * openai_valid_results.num_rows / inference_data.num_rows))\n",
    "print(\"Replicate valid results: {:.2f}%\".format(100 * replicate_valid_results.num_rows / inference_data.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Valid Queries', 'Valid Results']\n",
    "openai_values = [openai_valid_queries.num_rows, openai_valid_results.num_rows]\n",
    "replicate_values = [replicate_valid_queries.num_rows, replicate_valid_results.num_rows]\n",
    "\n",
    "openai_percentages = [100 * openai_valid_queries.num_rows / inference_data.num_rows,\n",
    "                      100 * openai_valid_results.num_rows / inference_data.num_rows]\n",
    "\n",
    "replicate_percentages = [100 * replicate_valid_queries.num_rows / inference_data.num_rows,\n",
    "                         100 * replicate_valid_results.num_rows / inference_data.num_rows]\n",
    "\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar charts for absolute values\n",
    "ax1.bar(x - width/2, openai_values, width, color='tab:blue', label='OpenAI')\n",
    "ax1.bar(x + width/2, replicate_values, width, color='tab:red', label='Replicate')\n",
    "\n",
    "# Labeling and other aesthetics\n",
    "ax1.set_xlabel('Metrics')\n",
    "ax1.set_ylabel('Counts')\n",
    "ax1.set_title('Comparison of OpenAI vs Replicate')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(categories)\n",
    "ax1.legend()\n",
    "\n",
    "# Adding percentages on top of the bars\n",
    "for i in range(len(categories)):\n",
    "    ax1.text(i - width/2, openai_values[i] + 5, f\"{openai_percentages[i]:.2f}%\", ha='center', va='bottom', color='black', rotation=0)\n",
    "    ax1.text(i + width/2, replicate_values[i] + 5, f\"{replicate_percentages[i]:.2f}%\", ha='center', va='bottom', color='black', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosql",
   "language": "python",
   "name": "autosql"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
