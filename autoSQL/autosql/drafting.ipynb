{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot.executor import execute\n",
    "from sqlglot.errors import ExecuteError, TokenError, SchemaError, ExecuteError, ParseError, UnsupportedError, SqlglotError\n",
    "\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from faker import Faker\n",
    "import json\n",
    "\n",
    "# Initialize a Faker instance\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('b-mc2/sql-create-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a new column\n",
    "\n",
    "```python\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "```\n",
    "\n",
    "alternatively \n",
    "\n",
    "```python \n",
    "drug_dataset = drug_dataset.add_column(list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_table_count(dataset): \n",
    "    count = len(dataset['context'].split(';'))\n",
    "    return {\"table_count\": count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_column_types(dataset): \n",
    "    \n",
    "    atls = sqlglot.parse(dataset['context'])\n",
    "    tables = {}\n",
    "\n",
    "    for atl in atls:\n",
    "        column_types = {}\n",
    "        table_name = atl.find(sqlglot.expressions.Identifier).this\n",
    "        for expr in atl.this.expressions:\n",
    "            column_name = expr.find(sqlglot.expressions.Identifier).this\n",
    "            column_type = expr.find(sqlglot.expressions.DataType).this.value\n",
    "            column_types[column_name] = column_type\n",
    "        tables[table_name] = column_types\n",
    "\n",
    "    return {\"column_types\": json.dumps(tables)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_duplicate_create_table(dataset): \n",
    "\n",
    "    create_count = dataset['table_count']\n",
    "    table_count = len(json.loads(dataset['column_types']).keys())\n",
    "\n",
    "    if create_count == table_count:\n",
    "        return {\"duplicate_create_table\": False}\n",
    "    else:\n",
    "        return {\"duplicate_create_table\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your column types data\n",
    "column_types_data = '{\"station\": {\"name\": \"VARCHAR\", \"id\": \"VARCHAR\", \"installation_date\": \"VARCHAR\"}, \"status\": {\"station_id\": \"VARCHAR\", \"bikes_available\": \"INT\"}}'\n",
    "column_types = json.loads(column_types_data)\n",
    "\n",
    "def generate_random_data(data_type):\n",
    "    if data_type == \"VARCHAR\":\n",
    "        return fake.name()\n",
    "    elif data_type == \"INT\":\n",
    "        return fake.random_int(min=1, max=100)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_filler_data(column_types, num_records=5):\n",
    "    filler_data = {}\n",
    "    \n",
    "    for table_name, columns in column_types.items():\n",
    "        filler_data[table_name] = []\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            record = {}\n",
    "            for column_name, data_type in columns.items():\n",
    "                record[column_name] = generate_random_data(data_type)\n",
    "            filler_data[table_name].append(record)\n",
    "    \n",
    "    return filler_data\n",
    "\n",
    "def populate_data(dataset): \n",
    "\n",
    "    column_types = json.loads(dataset['column_types'])\n",
    "    return {\"filler_data\": json.dumps(generate_filler_data(column_types))}\n",
    "\n",
    "# Generate filler data\n",
    "filler_data = generate_filler_data(column_types)\n",
    "print(json.dumps(filler_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def validate_query(dataset): \n",
    "\n",
    "    tables = json.loads(dataset['filler_data'])\n",
    "    query = dataset['answer']\n",
    "\n",
    "    try:\n",
    "        result = execute(query, tables)\n",
    "        return {\"valid_query\": True}\n",
    "    except:\n",
    "        return {\"valid_query\": False}\n",
    "'''\n",
    "    \n",
    "def validate_query(dataset): \n",
    "\n",
    "    tables = json.loads(dataset['filler_data'])\n",
    "    query = dataset['answer']\n",
    "\n",
    "    try:\n",
    "        result = execute(query, tables=tables)\n",
    "        result = str(result.rows)\n",
    "        if result == None:\n",
    "            result = ''\n",
    "        return {\"query_result\": result, \"valid_query\": True}\n",
    "    except ExecuteError as e:\n",
    "        return {\"query_result\": \"ExecuteError\", \"valid_query\": False}\n",
    "    except TokenError as e:\n",
    "        return {\"query_result\": \"TokenError\", \"valid_query\": False}\n",
    "    except SchemaError as e:\n",
    "        return {\"query_result\": \"SchemaError\", \"valid_query\": False}\n",
    "    except ParseError as e:\n",
    "        return {\"query_result\": \"ParseError\", \"valid_query\": False}\n",
    "    except UnsupportedError as e:\n",
    "        return {\"query_result\": \"UnsupportedError\", \"valid_query\": False}\n",
    "    except SqlglotError as e: # it seems like this one gets thrown a lot even when its correct \n",
    "        return {\"query_result\": \"SqlglotError\", \"valid_query\": True}\n",
    "    except Exception as e:\n",
    "        return {\"query_result\": e, \"valid_query\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(compute_table_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(abstract_column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(identify_duplicate_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(populate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(validate_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a count of invalid queries\n",
    "invalid_queries = dataset['train'].filter(lambda x: x['valid_query'] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_queries[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoSQL",
   "language": "python",
   "name": "autosql"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
